{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709db37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from gensim import corpora\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import pos_tag\n",
    "import string\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960130ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download required NLTK data files\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# Sample data\n",
    "issues = [\n",
    "    \"Seems like this particular input is confusing the sentence boundary detection (full stop prediction) head. Note how the first instance of `thank` was capitalized after the comma, too. The model seems confused as to where the sentences end (likely due to repeated texts which are unusual of the training data). I would recommend the better model, This is the output I get with that one: ```text After all, isn't that what we're here for? \\n That's why we're here for. \\n Well, Rob, you are an inspiration for the nation. \\n Thank you for coming on, thank you for having me. \\n Thank you so much for watching this week's episode. ```\"\n",
    "]\n",
    "\n",
    "# Preprocessing function to extract meaningful key phrases\n",
    "def preprocess(text):\n",
    "    # Tokenize text\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    # Remove punctuation and stop words\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    # Perform Part-of-Speech tagging\n",
    "    tagged_tokens = pos_tag(tokens)\n",
    "    # Select specific types of words (nouns and verbs)\n",
    "    meaningful_tokens = [word for word, pos in tagged_tokens if pos.startswith('NN') or pos.startswith('VB')]\n",
    "    # Extract first two meaningful tokens, or all if fewer than two meaningful tokens are found\n",
    "    meaningful_tokens = meaningful_tokens[:2]\n",
    "    return meaningful_tokens\n",
    "\n",
    "# Preprocess issues to extract meaningful key phrases\n",
    "processed_issues = [preprocess(issue) for issue in issues]\n",
    "\n",
    "# Create dictionary and corpus for gensim\n",
    "dictionary = corpora.Dictionary(processed_issues)\n",
    "corpus = [dictionary.doc2bow(issue) for issue in processed_issues]\n",
    "\n",
    "# Print processed data\n",
    "print(\"Processed Issues (Meaningful key phrases):\", processed_issues)\n",
    "print(\"Dictionary:\", dictionary)\n",
    "print(\"Corpus:\", corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4482e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from gensim import corpora\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import nltk\n",
    "import spacy\n",
    "\n",
    "# Download required NLTK data files\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Sample data\n",
    "issues = [\n",
    "    \"Seems like this particular input is confusing the sentence boundary detection (full stop prediction) head. Note how the first instance of `thank` was capitalized after the comma, too. The model seems confused as to where the sentences end (likely due to repeated texts which are unusual of the training data). I would recommend the better model, This is the output I get with that one: ```text After all, isn't that what we're here for? \\n That's why we're here for. \\n Well, Rob, you are an inspiration for the nation. \\n Thank you for coming on, thank you for having me. \\n Thank you so much for watching this week's episode. ```\"\n",
    "]\n",
    "\n",
    "# Load English language model in spaCy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Preprocessing function using Dependency Parsing\n",
    "def preprocess(text):\n",
    "    # Tokenize text using spaCy\n",
    "    doc = nlp(text)\n",
    "    # Extract meaningful phrases based on dependency parsing\n",
    "    meaningful_tokens = []\n",
    "    for token in doc:\n",
    "        if token.dep_ in ['nsubj', 'dobj', 'pobj']:  # Include subject, direct object, and object of preposition\n",
    "            meaningful_tokens.append(token.text)\n",
    "    # Extract first two meaningful tokens, or all if fewer than two meaningful tokens are found\n",
    "    meaningful_tokens = meaningful_tokens[:2]\n",
    "    return meaningful_tokens\n",
    "\n",
    "# Preprocess issues to extract meaningful key phrases using Dependency Parsing\n",
    "processed_issues = [preprocess(issue) for issue in issues]\n",
    "\n",
    "# Create dictionary and corpus for gensim\n",
    "dictionary = corpora.Dictionary(processed_issues)\n",
    "corpus = [dictionary.doc2bow(issue) for issue in processed_issues]\n",
    "\n",
    "# Print processed data\n",
    "print(\"Processed Issues (Meaningful key phrases):\", processed_issues)\n",
    "print(\"Dictionary:\", dictionary)\n",
    "print(\"Corpus:\", corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81c23f69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Issues (Meaningful key phrases): ['looking model', 'model data']\n",
      "Dictionary: Dictionary<2 unique tokens: ['looking model', 'model data']>\n",
      "Corpus: [[(0, 1), (1, 1)]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/adekunleajibode/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/adekunleajibode/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from gensim import corpora\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "# Download required NLTK data files\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def clean_commit_message(commit_message):\n",
    "    # Remove emojis and other symbols\n",
    "    cleaned_message = re.sub(r'[^\\x00-\\x7F]+', ' ', commit_message)\n",
    "    # Remove quotes\n",
    "    cleaned_message = re.sub(r'\"(.*?)\"', '', cleaned_message)\n",
    "    # Remove leading and trailing whitespace\n",
    "    cleaned_message = cleaned_message.strip()\n",
    "    return cleaned_message\n",
    "\n",
    "# Example commit message\n",
    "commit_message = \"\"\"\n",
    "I am looking for this model to fine-tune my own data (such as medical science) and after the training, I want it to be able to answer the questions. Then I am not looking for the \"extractive answers\" where it returns the start and end sequence (which is pretty much related the given context scenario) but a \"generative case\" where I train the model with my data and then answer the question, and from its (the model's) own understanding for my data, it should be able to give me the answers. If anybody knows how to achieve that with this model, please let me know! Thank you so much ü§ó\n",
    "\"\"\"\n",
    "\n",
    "# Clean the commit message\n",
    "cleaned_message = clean_commit_message(commit_message)\n",
    "\n",
    "# Preprocessing function using POS tagging and refined heuristic approach\n",
    "def preprocess(text):\n",
    "    # Tokenize text\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    # Remove punctuation and stop words\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    # Perform Part-of-Speech tagging\n",
    "    tagged_tokens = nltk.pos_tag(tokens)\n",
    "    # Heuristic approach to select meaningful phrases\n",
    "    meaningful_tokens = []\n",
    "    for i in range(len(tagged_tokens) - 1):\n",
    "        if tagged_tokens[i][1].startswith('NN') and tagged_tokens[i + 1][1].startswith('NN'):\n",
    "            meaningful_tokens.append(tagged_tokens[i][0] + ' ' + tagged_tokens[i + 1][0])\n",
    "        elif tagged_tokens[i][1].startswith('VB') and tagged_tokens[i + 1][1].startswith('NN'):\n",
    "            meaningful_tokens.append(tagged_tokens[i][0] + ' ' + tagged_tokens[i + 1][0])\n",
    "    # Extract first two meaningful tokens, or all if fewer than two meaningful tokens are found\n",
    "    meaningful_tokens = meaningful_tokens[:2]\n",
    "    # If no meaningful tokens are found, return an empty list\n",
    "    if not meaningful_tokens:\n",
    "        meaningful_tokens = []\n",
    "    return meaningful_tokens\n",
    "\n",
    "# Preprocess cleaned commit message to extract meaningful key phrases using POS tagging and refined heuristic approach\n",
    "processed_issues = preprocess(cleaned_message)\n",
    "\n",
    "# Create dictionary and corpus for gensim (for demonstration)\n",
    "dictionary = corpora.Dictionary([processed_issues])\n",
    "corpus = [dictionary.doc2bow(processed_issues)]\n",
    "\n",
    "# Print processed data\n",
    "print(\"Processed Issues (Meaningful key phrases):\", processed_issues)\n",
    "print(\"Dictionary:\", dictionary)\n",
    "print(\"Corpus:\", corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a01d979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting summa\n",
      "  Downloading summa-1.2.0.tar.gz (54 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.9/54.9 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: scipy>=0.19 in /Users/adekunleajibode/anaconda3/lib/python3.11/site-packages (from summa) (1.10.1)\n",
      "Requirement already satisfied: numpy<1.27.0,>=1.19.5 in /Users/adekunleajibode/anaconda3/lib/python3.11/site-packages (from scipy>=0.19->summa) (1.24.3)\n",
      "Building wheels for collected packages: summa\n",
      "  Building wheel for summa (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for summa: filename=summa-1.2.0-py3-none-any.whl size=54388 sha256=6939ccd536201a226c410d52a9b42fd497dd234c5589be3298bdf16ac2e5195a\n",
      "  Stored in directory: /Users/adekunleajibode/Library/Caches/pip/wheels/10/2d/7a/abce87c4ea233f8dcca0d99b740ac0257eced1f99a124a0e1f\n",
      "Successfully built summa\n",
      "Installing collected packages: summa\n",
      "Successfully installed summa-1.2.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install summa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "23443266",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Issues (Meaningful key phrases): ['scratch hf']\n",
      "Dictionary: Dictionary<1 unique tokens: ['scratch hf']>\n",
      "Corpus: [[(0, 1)]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/adekunleajibode/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/adekunleajibode/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/adekunleajibode/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from gensim import corpora\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "# Download required NLTK data files\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "def clean_commit_message(commit_message):\n",
    "    # Remove emojis and other symbols\n",
    "    cleaned_message = re.sub(r'[^\\x00-\\x7F]+', ' ', commit_message)\n",
    "    # Remove quotes\n",
    "    cleaned_message = re.sub(r'\"(.*?)\"', '', cleaned_message)\n",
    "    # Remove leading and trailing whitespace\n",
    "    cleaned_message = cleaned_message.strip()\n",
    "    return cleaned_message\n",
    "\n",
    "# Example commit message\n",
    "commit_message = \"\"\"\n",
    "Would be great for this model to be citable ! Side question: was this trained from scratch by HF? Is there an original paper to be cited if we use this in a publication?\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Clean the commit message\n",
    "cleaned_message = clean_commit_message(commit_message)\n",
    "\n",
    "# Function to preprocess and extract meaningful key phrases using NLTK\n",
    "def preprocess(text):\n",
    "    # Tokenize text\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    # Remove punctuation and stop words\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    # Perform Part-of-Speech tagging\n",
    "    tagged_tokens = nltk.pos_tag(tokens)\n",
    "    # Heuristic approach to select meaningful phrases\n",
    "    meaningful_tokens = []\n",
    "    for i in range(len(tagged_tokens) - 1):\n",
    "        if (tagged_tokens[i][1].startswith('NN') or tagged_tokens[i][1].startswith('NN')) and (\n",
    "            tagged_tokens[i + 1][1].startswith('NN') or tagged_tokens[i + 1][1].startswith('NN')):\n",
    "            meaningful_tokens.append(tagged_tokens[i][0] + ' ' + tagged_tokens[i + 1][0])\n",
    "    # Extract first two meaningful tokens, or all if fewer than two meaningful tokens are found\n",
    "    meaningful_tokens = meaningful_tokens[:2]\n",
    "    return meaningful_tokens\n",
    "\n",
    "# Preprocess and extract meaningful key phrases using NLTK\n",
    "processed_issues = preprocess(cleaned_message)\n",
    "\n",
    "# Create dictionary and corpus for gensim (for demonstration)\n",
    "dictionary = corpora.Dictionary([processed_issues])\n",
    "corpus = [dictionary.doc2bow(processed_issues)]\n",
    "\n",
    "# Print processed data\n",
    "print(\"Processed Issues (Meaningful key phrases):\", processed_issues)\n",
    "print(\"Dictionary:\", dictionary)\n",
    "print(\"Corpus:\", corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ae7e71f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Issues (Meaningful key phrases): ['trained scratch', 'scratch hf']\n",
      "Dictionary: Dictionary<2 unique tokens: ['scratch hf', 'trained scratch']>\n",
      "Corpus: [[(0, 1), (1, 1)]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/adekunleajibode/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/adekunleajibode/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/adekunleajibode/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from gensim import corpora\n",
    "\n",
    "# Download NLTK data if not already downloaded\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "def clean_commit_message(commit_message):\n",
    "    # Remove emojis and other symbols\n",
    "    cleaned_message = re.sub(r'[^\\x00-\\x7F]+', ' ', commit_message)\n",
    "    # Remove quotes\n",
    "    cleaned_message = re.sub(r'\"(.*?)\"', '', cleaned_message)\n",
    "    # Remove leading and trailing whitespace\n",
    "    cleaned_message = cleaned_message.strip()\n",
    "    return cleaned_message\n",
    "\n",
    "# Example commit message\n",
    "commit_message = \"\"\"\n",
    "Would be great for this model to be citable ! Side question: was this trained from scratch by HF? Is there an original paper to be cited if we use this in a publication?\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Clean the commit message\n",
    "cleaned_message = clean_commit_message(commit_message)\n",
    "\n",
    "# Function to preprocess and extract meaningful key phrases using NLTK\n",
    "def preprocess(text):\n",
    "    # Tokenize text\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    # Remove punctuation and stop words\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    # Perform Part-of-Speech tagging\n",
    "    tagged_tokens = nltk.pos_tag(tokens)\n",
    "    # Heuristic approach to select meaningful phrases\n",
    "    meaningful_tokens = []\n",
    "    for i in range(len(tagged_tokens) - 1):\n",
    "        if (tagged_tokens[i][1].startswith('NN') and tagged_tokens[i + 1][1].startswith('NN')):  # Noun-Noun pattern\n",
    "            meaningful_tokens.append(tagged_tokens[i][0] + ' ' + tagged_tokens[i + 1][0])\n",
    "        elif (tagged_tokens[i][1].startswith('VB') and tagged_tokens[i + 1][1].startswith('NN')):  # Verb-Noun pattern\n",
    "            meaningful_tokens.append(tagged_tokens[i][0] + ' ' + tagged_tokens[i + 1][0])\n",
    "        # Add more patterns as needed (e.g., Noun-Adjective, etc.)\n",
    "    # Extract first two meaningful tokens, or all if fewer than two meaningful tokens are found\n",
    "    meaningful_tokens = meaningful_tokens[:2]\n",
    "    return meaningful_tokens\n",
    "\n",
    "# Preprocess and extract meaningful key phrases using NLTK\n",
    "processed_issues = preprocess(cleaned_message)\n",
    "\n",
    "# Create dictionary and corpus for gensim (for demonstration)\n",
    "dictionary = corpora.Dictionary([processed_issues])\n",
    "corpus = [dictionary.doc2bow(processed_issues)]\n",
    "\n",
    "# Print processed data\n",
    "print(\"Processed Issues (Meaningful key phrases):\", processed_issues)\n",
    "print(\"Dictionary:\", dictionary)\n",
    "print(\"Corpus:\", corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e6a7ecb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Issues (Meaningful key phrases): ['trained scratch', 'scratch hf']\n",
      "Dictionary: Dictionary<2 unique tokens: ['scratch hf', 'trained scratch']>\n",
      "Corpus: [[(0, 1), (1, 1)]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/adekunleajibode/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/adekunleajibode/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/adekunleajibode/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from gensim import corpora\n",
    "\n",
    "# Download NLTK data if not already downloaded\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "def clean_commit_message(commit_message):\n",
    "    # Remove emojis and other symbols\n",
    "    cleaned_message = re.sub(r'[^\\x00-\\x7F]+', ' ', commit_message)\n",
    "    # Remove quotes\n",
    "    cleaned_message = re.sub(r'\"(.*?)\"', '', cleaned_message)\n",
    "    # Remove leading and trailing whitespace\n",
    "    cleaned_message = cleaned_message.strip()\n",
    "    return cleaned_message\n",
    "\n",
    "# Example commit message\n",
    "commit_message = \"\"\"\n",
    "Would be great for this model to be citable ! Side question: was this trained from scratch by HF? Is there an original paper to be cited if we use this in a publication?\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Clean the commit message\n",
    "cleaned_message = clean_commit_message(commit_message)\n",
    "\n",
    "# Function to preprocess and extract meaningful key phrases using NLTK\n",
    "def preprocess(text):\n",
    "    # Tokenize text\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    # Remove punctuation and stop words\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    # Perform Part-of-Speech tagging\n",
    "    tagged_tokens = nltk.pos_tag(tokens)\n",
    "    # Heuristic approach to select meaningful phrases\n",
    "    meaningful_tokens = []\n",
    "    for i in range(len(tagged_tokens) - 1):\n",
    "        if (tagged_tokens[i][1].startswith('NN') and tagged_tokens[i + 1][1].startswith('NN')):  # Noun-Noun pattern\n",
    "            meaningful_tokens.append(tagged_tokens[i][0] + ' ' + tagged_tokens[i + 1][0])\n",
    "        elif (tagged_tokens[i][1].startswith('VB') and tagged_tokens[i + 1][1].startswith('NN')):  # Verb-Noun pattern\n",
    "            meaningful_tokens.append(tagged_tokens[i][0] + ' ' + tagged_tokens[i + 1][0])\n",
    "        # Add more patterns as needed (e.g., Noun-Adjective, etc.)\n",
    "    # Extract first two meaningful tokens, or all if fewer than two meaningful tokens are found\n",
    "    meaningful_tokens = meaningful_tokens[:2]\n",
    "    return meaningful_tokens\n",
    "\n",
    "# Preprocess and extract meaningful key phrases using NLTK\n",
    "processed_issues = preprocess(cleaned_message)\n",
    "\n",
    "# Create dictionary and corpus for gensim (for demonstration)\n",
    "dictionary = corpora.Dictionary([processed_issues])\n",
    "corpus = [dictionary.doc2bow(processed_issues)]\n",
    "\n",
    "# Print processed data\n",
    "print(\"Processed Issues (Meaningful key phrases):\", processed_issues)\n",
    "print(\"Dictionary:\", dictionary)\n",
    "print(\"Corpus:\", corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52df27e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
