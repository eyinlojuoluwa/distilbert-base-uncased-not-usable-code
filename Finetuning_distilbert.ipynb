{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3b475ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import re\n",
    "from transformers import AutoTokenizer, DistilBertTokenizer, DistilBertForSequenceClassification, AutoModelForSequenceClassification, Trainer, TrainingArguments, DistilBertConfig\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import wordnet\n",
    "import random\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import nltk\n",
    "from transformers import DistilBertTokenizer\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5bbbaf5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "external documentation       501\n",
       "model structure              195\n",
       "project metadata             141\n",
       "sharing                      127\n",
       "preprocessing                 61\n",
       "training infrastructure       55\n",
       "validation infrastructure     52\n",
       "input data                    35\n",
       "internal documentation        35\n",
       "pipeline performance          33\n",
       "parameter tuning              31\n",
       "add dependency                19\n",
       "output data                   18\n",
       "update dependency             17\n",
       "remove dependency             15\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"dataset/combined_GH_HF_manual.csv\")\n",
    "\n",
    "# Function to clean text columns\n",
    "def clean_text(text):\n",
    "    # Remove non-ASCII characters (corrupted/malformed characters)\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)  # Remove non-ASCII characters\n",
    "    return text.strip()\n",
    "\n",
    "# Apply cleaning function to relevant columns\n",
    "df['message'] = df['message'].apply(clean_text)\n",
    "df = df[df['message'].notna() & (df['message'] != '') & (df['message'].str.split().str.len() > 1)]\n",
    "df[\"label\"] = df[\"label\"].str.lower()\n",
    "\n",
    "number_of_labels = df[\"label\"].value_counts()\n",
    "number_of_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "222f826b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'add dependency', 1: 'external documentation', 2: 'input data', 3: 'internal documentation', 4: 'model structure', 5: 'output data', 6: 'parameter tuning', 7: 'pipeline performance', 8: 'preprocessing', 9: 'project metadata', 10: 'remove dependency', 11: 'sharing', 12: 'training infrastructure', 13: 'update dependency', 14: 'validation infrastructure'}\n",
      "Training data: 1092\n",
      "Validation data: 277\n",
      "Test data: 12\n",
      "1092\n",
      "277\n",
      "12\n",
      "{'input_ids': tensor([  101, 10651,  3808,  1035,  3556,  1012,  1052,  2100,   102,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0]), 'labels': tensor(14)}\n"
     ]
    }
   ],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(df[\"label\"])  # Fit on the original labels\n",
    "\n",
    "df['encoded_labels'] = label_encoder.fit_transform(df['label'])  # Encode the labels as integers\n",
    "label_mapping = {index: label for index, label in enumerate(label_encoder.classes_)}\n",
    "print(label_mapping)\n",
    "\n",
    "# Now split the dataset\n",
    "data_texts = df['message'].tolist()  # Your text data\n",
    "data_labels = df['encoded_labels'].tolist()  # Your encoded integer labels\n",
    "\n",
    "# Split into Train and Validation (80% train, 20% validation)\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(data_texts, data_labels, test_size=0.2, random_state=0, shuffle=True)\n",
    "\n",
    "# Further split Train data into Train and Test (99% train, 1% test)\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(train_texts, train_labels, test_size=0.01, random_state=0, shuffle=True)\n",
    "\n",
    "# Check the number of items in each split\n",
    "print(f\"Training data: {len(train_texts)}\")\n",
    "print(f\"Validation data: {len(val_texts)}\")\n",
    "print(f\"Test data: {len(test_texts)}\")\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# Preprocessing function to tokenize the data\n",
    "def preprocess_function(texts):\n",
    "    return tokenizer(texts, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# Tokenize the train, validation, and test data\n",
    "train_encodings = preprocess_function(train_texts)\n",
    "val_encodings = preprocess_function(val_texts)\n",
    "test_encodings = preprocess_function(test_texts)\n",
    "\n",
    "# Check the lengths of the encoded data\n",
    "print(len(train_encodings[\"input_ids\"]))  # Should match len(train_texts)\n",
    "print(len(val_encodings[\"input_ids\"]))    # Should match len(val_texts)\n",
    "print(len(test_encodings[\"input_ids\"]))   # Should match len(test_texts)\n",
    "\n",
    "# Now create the datasets with integer labels\n",
    "train_dataset = [{\n",
    "    \"input_ids\": enc, \n",
    "    \"attention_mask\": train_encodings[\"attention_mask\"][i], \n",
    "    \"labels\": torch.tensor(train_labels[i])  # Ensure labels are a tensor (integers)\n",
    "} for i, enc in enumerate(train_encodings[\"input_ids\"])]\n",
    "\n",
    "val_dataset = [{\n",
    "    \"input_ids\": enc, \n",
    "    \"attention_mask\": val_encodings[\"attention_mask\"][i], \n",
    "    \"labels\": torch.tensor(val_labels[i])  # Ensure labels are a tensor (integers)\n",
    "} for i, enc in enumerate(val_encodings[\"input_ids\"])]\n",
    "\n",
    "test_dataset = [{\n",
    "    \"input_ids\": enc, \n",
    "    \"attention_mask\": test_encodings[\"attention_mask\"][i], \n",
    "    \"labels\": torch.tensor(test_labels[i])  # Ensure labels are a tensor (integers)\n",
    "} for i, enc in enumerate(test_encodings[\"input_ids\"])]\n",
    "\n",
    "# Check the first tokenized data example from the train dataset\n",
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "794fd247",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-08 23:12:57,737] A new study created in memory with name: no-name-20a03cdb-950e-4746-8db8-01bb44690acb\n",
      "/var/folders/4p/p4dff74s5rb42f27sxjfg97r0000gn/T/ipykernel_77041/636126082.py:24: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 2e-5)\n",
      "/var/folders/4p/p4dff74s5rb42f27sxjfg97r0000gn/T/ipykernel_77041/636126082.py:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  weight_decay = trial.suggest_loguniform('weight_decay', 1e-4, 0.1)  # Now tuning weight decay\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/adekunleajibode/anaconda3/lib/python3.11/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='931' max='931' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [931/931 24:25, Epoch 7/7]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.682000</td>\n",
       "      <td>1.488799</td>\n",
       "      <td>0.599251</td>\n",
       "      <td>0.789984</td>\n",
       "      <td>0.599251</td>\n",
       "      <td>0.524755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.407000</td>\n",
       "      <td>1.272926</td>\n",
       "      <td>0.651685</td>\n",
       "      <td>0.783567</td>\n",
       "      <td>0.651685</td>\n",
       "      <td>0.587409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.141000</td>\n",
       "      <td>1.204599</td>\n",
       "      <td>0.647940</td>\n",
       "      <td>0.733516</td>\n",
       "      <td>0.647940</td>\n",
       "      <td>0.577464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.034900</td>\n",
       "      <td>1.116121</td>\n",
       "      <td>0.681648</td>\n",
       "      <td>0.761606</td>\n",
       "      <td>0.681648</td>\n",
       "      <td>0.622491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.924700</td>\n",
       "      <td>1.085882</td>\n",
       "      <td>0.696629</td>\n",
       "      <td>0.755504</td>\n",
       "      <td>0.696629</td>\n",
       "      <td>0.644554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.017200</td>\n",
       "      <td>1.078118</td>\n",
       "      <td>0.692884</td>\n",
       "      <td>0.767491</td>\n",
       "      <td>0.692884</td>\n",
       "      <td>0.632926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.944200</td>\n",
       "      <td>1.072241</td>\n",
       "      <td>0.689139</td>\n",
       "      <td>0.726281</td>\n",
       "      <td>0.689139</td>\n",
       "      <td>0.633593</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='34' max='34' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [34/34 00:14]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-08 23:37:40,934] Trial 0 finished with value: 1.0722405910491943 and parameters: {'learning_rate': 1.6197048683804554e-05, 'batch_size': 8, 'weight_decay': 0.020932077220795306}. Best is trial 0 with value: 1.0722405910491943.\n",
      "/var/folders/4p/p4dff74s5rb42f27sxjfg97r0000gn/T/ipykernel_77041/636126082.py:24: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 2e-5)\n",
      "/var/folders/4p/p4dff74s5rb42f27sxjfg97r0000gn/T/ipykernel_77041/636126082.py:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  weight_decay = trial.suggest_loguniform('weight_decay', 1e-4, 0.1)  # Now tuning weight decay\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/adekunleajibode/anaconda3/lib/python3.11/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='931' max='931' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [931/931 26:34, Epoch 7/7]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.753900</td>\n",
       "      <td>1.582125</td>\n",
       "      <td>0.576779</td>\n",
       "      <td>0.727629</td>\n",
       "      <td>0.576779</td>\n",
       "      <td>0.482579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.455200</td>\n",
       "      <td>1.317827</td>\n",
       "      <td>0.621723</td>\n",
       "      <td>0.727495</td>\n",
       "      <td>0.621723</td>\n",
       "      <td>0.545050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.223800</td>\n",
       "      <td>1.215786</td>\n",
       "      <td>0.640449</td>\n",
       "      <td>0.725154</td>\n",
       "      <td>0.640449</td>\n",
       "      <td>0.574752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.074500</td>\n",
       "      <td>1.141149</td>\n",
       "      <td>0.662921</td>\n",
       "      <td>0.746711</td>\n",
       "      <td>0.662921</td>\n",
       "      <td>0.595950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.012800</td>\n",
       "      <td>1.119750</td>\n",
       "      <td>0.689139</td>\n",
       "      <td>0.758895</td>\n",
       "      <td>0.689139</td>\n",
       "      <td>0.640064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.998100</td>\n",
       "      <td>1.103964</td>\n",
       "      <td>0.677903</td>\n",
       "      <td>0.736976</td>\n",
       "      <td>0.677903</td>\n",
       "      <td>0.616111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.942800</td>\n",
       "      <td>1.098190</td>\n",
       "      <td>0.677903</td>\n",
       "      <td>0.736976</td>\n",
       "      <td>0.677903</td>\n",
       "      <td>0.616111</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='34' max='34' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [34/34 00:16]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-09 00:04:34,798] Trial 1 finished with value: 1.0981897115707397 and parameters: {'learning_rate': 1.4573014364047723e-05, 'batch_size': 8, 'weight_decay': 0.0004745658869563308}. Best is trial 0 with value: 1.0722405910491943.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'learning_rate': 1.6197048683804554e-05, 'batch_size': 8, 'weight_decay': 0.020932077220795306}\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "import numpy as np\n",
    "from transformers import Trainer, TrainingArguments, DistilBertForSequenceClassification, DistilBertTokenizer, EarlyStoppingCallback\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "# Define the compute_metrics function\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=1)\n",
    "\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        labels, predictions, average='weighted', zero_division=1\n",
    "    )\n",
    "    acc = accuracy_score(labels, predictions)\n",
    "\n",
    "    return {\"accuracy\": acc, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# Define the objective function for Optuna optimization\n",
    "def objective(trial):\n",
    "    # Suggest hyperparameters\n",
    "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 2e-5)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [8, 16, 32])\n",
    "    weight_decay = trial.suggest_loguniform('weight_decay', 1e-4, 0.1)  # Now tuning weight decay\n",
    "\n",
    "    # Reinitialize model for each trial to avoid contamination\n",
    "    model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=15)\n",
    "\n",
    "    # Set training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./results\",\n",
    "        learning_rate=learning_rate,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,  # Ensure eval batch size matches\n",
    "        num_train_epochs=7,\n",
    "        weight_decay=weight_decay,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        logging_dir=\"./logs\",\n",
    "        logging_steps=10,\n",
    "        save_strategy=\"epoch\",  # Save best model per epoch\n",
    "        load_best_model_at_end=True,  # Load the best model automatically\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "        greater_is_better=False,  # We want lower validation loss\n",
    "    )\n",
    "\n",
    "    # Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=2)],  # Stop if no improvement in 2 epochs\n",
    "    )\n",
    "\n",
    "    # Train and return validation loss\n",
    "    trainer.train()\n",
    "    eval_results = trainer.evaluate()\n",
    "    return eval_results['eval_loss']\n",
    "\n",
    "# Optimize the hyperparameters\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=2)\n",
    "\n",
    "# Output the best hyperparameters found\n",
    "print(\"Best hyperparameters:\", study.best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f151b431",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='931' max='931' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [931/931 24:58, Epoch 7/7]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.682100</td>\n",
       "      <td>1.516419</td>\n",
       "      <td>0.588015</td>\n",
       "      <td>0.785686</td>\n",
       "      <td>0.588015</td>\n",
       "      <td>0.521193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.416100</td>\n",
       "      <td>1.280272</td>\n",
       "      <td>0.640449</td>\n",
       "      <td>0.789482</td>\n",
       "      <td>0.640449</td>\n",
       "      <td>0.585507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.100400</td>\n",
       "      <td>1.199767</td>\n",
       "      <td>0.651685</td>\n",
       "      <td>0.729742</td>\n",
       "      <td>0.651685</td>\n",
       "      <td>0.583370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.048300</td>\n",
       "      <td>1.107124</td>\n",
       "      <td>0.681648</td>\n",
       "      <td>0.775472</td>\n",
       "      <td>0.681648</td>\n",
       "      <td>0.621044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.945300</td>\n",
       "      <td>1.087859</td>\n",
       "      <td>0.692884</td>\n",
       "      <td>0.745484</td>\n",
       "      <td>0.692884</td>\n",
       "      <td>0.642730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.978400</td>\n",
       "      <td>1.064542</td>\n",
       "      <td>0.696629</td>\n",
       "      <td>0.739138</td>\n",
       "      <td>0.696629</td>\n",
       "      <td>0.643155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.922000</td>\n",
       "      <td>1.063261</td>\n",
       "      <td>0.692884</td>\n",
       "      <td>0.735425</td>\n",
       "      <td>0.692884</td>\n",
       "      <td>0.639714</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=931, training_loss=1.2256117642373967, metrics={'train_runtime': 1500.0455, 'train_samples_per_second': 4.933, 'train_steps_per_second': 0.621, 'total_flos': 980353511746560.0, 'train_loss': 1.2256117642373967, 'epoch': 7.0})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from transformers import Trainer, TrainingArguments, DistilBertForSequenceClassification\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "# Define the compute_metrics function\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=1)\n",
    "\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        labels, predictions, average='weighted', zero_division=1\n",
    "    )\n",
    "    acc = accuracy_score(labels, predictions)\n",
    "\n",
    "    return {\"accuracy\": acc, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
    "\n",
    "# Define the model\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=15)\n",
    "\n",
    "# Set up training arguments with best hyperparameters\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=best_params[\"learning_rate\"],\n",
    "    per_device_train_batch_size=best_params[\"per_device_train_batch_size\"],\n",
    "    num_train_epochs=10,  # Now set to final training epochs\n",
    "    weight_decay=best_params[\"weight_decay\"],\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10\n",
    ")\n",
    "\n",
    "# Subclass Trainer to remove weighted loss function\n",
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "\n",
    "        loss_fct = nn.CrossEntropyLoss()\n",
    "        loss = loss_fct(logits, labels)\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "# Instantiate and train with best hyperparameters\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=15)\n",
    "\n",
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a9dd95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_loss = []\n",
    "eval_loss = []\n",
    "epochs = []  # Store unique epoch numbers\n",
    "\n",
    "for log in trainer.state.log_history:\n",
    "    if \"loss\" in log and \"epoch\" in log:\n",
    "        train_loss.append(log[\"loss\"])\n",
    "    if \"eval_loss\" in log and \"epoch\" in log:\n",
    "        eval_loss.append(log[\"eval_loss\"])\n",
    "        epochs.append(log[\"epoch\"])  # Extract the actual epoch number\n",
    "\n",
    "# Plot the losses\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(range(1, len(train_loss) + 1), train_loss, label=\"Training Loss\", color=\"blue\", linestyle=\"--\")\n",
    "plt.plot(epochs, eval_loss, label=\"Validation Loss\", color=\"red\", marker=\"o\")\n",
    "\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training & Validation Loss per Epoch\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad78714",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = DistilBertConfig.from_pretrained(\"distilbert-base-uncased\", num_labels=15)\n",
    "\n",
    "# Initialize the tokenizer (use the same tokenizer used for training)\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# Save the model, tokenizer, and config\n",
    "trainer.save_model(\"./final_model\")  # Save the model\n",
    "tokenizer.save_pretrained(\"./final_model\")  # Save the tokenizer\n",
    "config.save_pretrained(\"./final_model\")  # Save the config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9129cb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the trainer to make predictions on the test dataset\n",
    "predictions = trainer.predict(test_dataset)\n",
    "\n",
    "# Get the predicted labels by taking the argmax of the logits\n",
    "predicted_labels = np.argmax(predictions.predictions, axis=1)\n",
    "\n",
    "# Get the true labels from the test dataset (same format as predicted_labels)\n",
    "true_labels = [example['labels'].item() for example in test_dataset]  # Extract the labels as integers\n",
    "\n",
    "# Generate and print the classification report\n",
    "print(classification_report(true_labels, predicted_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8e4272",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (true, pred) in enumerate(zip(true_labels, predicted_labels)):\n",
    "    if true != pred:\n",
    "        # Retrieve the text for the misclassified example\n",
    "        text = test_texts[i]\n",
    "        \n",
    "        print(f\"Example {i}:\")\n",
    "        print(f\"Text: {text}\")\n",
    "        print(f\"True Label: {true}, Predicted Label: {pred}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a00555b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'add dependency', 1: 'external documentation', 2: 'input data', 3: 'internal documentation', 4: 'model structure', 5: 'output data', 6: 'parameter tuning', 7: 'pipeline performance', 8: 'preprocessing', 9: 'project metadata', 10: 'remove dependency', 11: 'sharing', 12: 'training infrastructure', 13: 'update dependency', 14: 'validation infrastructure'}\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f53ce03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7d95f7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "âœ… Predictions saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# Hugging Face Prediction\n",
    "\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "\n",
    "# Detect if Apple M1/M2/M3 chip is available\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")  # Will print 'mps' if available, otherwise 'cpu'\n",
    "\n",
    "# Load the trained model and tokenizer\n",
    "model_path = \"./final_model\"\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(model_path)\n",
    "model = DistilBertForSequenceClassification.from_pretrained(model_path)\n",
    "model.to(device)  # Move model to CPU or MPS (Mac GPU)\n",
    "model.eval()  # Set model to evaluation mode\n",
    "\n",
    "# Load dataset\n",
    "HF = pd.read_csv(\"dataset/HF_commit_986.csv\")\n",
    "HF['combine_message'] = HF['title'] + ' ' + HF['message'].fillna(HF['title'])\n",
    "\n",
    "# Function to clean text\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)  # Remove non-ASCII characters\n",
    "    return text.strip()\n",
    "\n",
    "HF['combine_message'] = HF['combine_message'].apply(clean_text)\n",
    "HF = HF[HF['combine_message'].notna() & (HF['combine_message'] != '') & (HF['combine_message'].str.split().str.len() > 1)]\n",
    "\n",
    "# Define batch size (set lower for MacBook to avoid crashes)\n",
    "BATCH_SIZE = 128  \n",
    "\n",
    "# Tokenize dataset in batches\n",
    "def tokenize_batch(texts):\n",
    "    return tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# Create DataLoader for batching\n",
    "def create_dataloader(texts, batch_size=BATCH_SIZE):\n",
    "    inputs = tokenize_batch(texts)\n",
    "    dataset = TensorDataset(inputs[\"input_ids\"], inputs[\"attention_mask\"])\n",
    "    return DataLoader(dataset, batch_size=batch_size)\n",
    "\n",
    "# Predict function\n",
    "def predict_labels(texts):\n",
    "    dataloader = create_dataloader(texts)\n",
    "    all_predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids, attention_mask = [b.to(device) for b in batch]\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            predictions = torch.argmax(outputs.logits, dim=1).cpu().numpy()\n",
    "            all_predictions.extend(predictions)\n",
    "\n",
    "    return all_predictions\n",
    "\n",
    "# Run predictions in batches\n",
    "HF[\"predicted_label\"] = predict_labels(HF[\"combine_message\"].tolist())\n",
    "HF[\"predicted_label\"] = HF[\"predicted_label\"].map(label_mapping)\n",
    "\n",
    "\n",
    "# Save results\n",
    "HF.to_csv(\"HF_commit_986_predictions.csv\", index=False)\n",
    "\n",
    "print(\"âœ… Predictions saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2419214f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "âœ… Predictions saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# GitHub Prediction\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "\n",
    "# Detect if Apple M1/M2/M3 chip is available\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")  # Will print 'mps' if available, otherwise 'cpu'\n",
    "\n",
    "# Load the trained model and tokenizer\n",
    "model_path = \"./final_model\"\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(model_path)\n",
    "model = DistilBertForSequenceClassification.from_pretrained(model_path)\n",
    "model.to(device)  # Move model to CPU or MPS (Mac GPU)\n",
    "model.eval()  # Set model to evaluation mode\n",
    "\n",
    "# Load dataset\n",
    "GH = pd.read_csv(\"dataset/GitHub_commits.csv\")\n",
    "#HF['combine_message'] = HF['title'] + ' ' + HF['message'].fillna(HF['title'])\n",
    "\n",
    "# Function to clean text\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str):  # Ensure text is a string\n",
    "        return \"\"\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)  # Remove non-ASCII characters\n",
    "    return text.strip()\n",
    "\n",
    "GH['commit_message'] = GH['commit_message'].apply(clean_text)\n",
    "GH = GH[GH['commit_message'].notna() & (GH['commit_message'] != '') & (GH['commit_message'].str.split().str.len() > 1)]\n",
    "\n",
    "# Define batch size (set lower for MacBook to avoid crashes)\n",
    "BATCH_SIZE = 128  \n",
    "\n",
    "# Tokenize dataset in batches\n",
    "def tokenize_batch(texts):\n",
    "    return tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# Create DataLoader for batching\n",
    "def create_dataloader(texts, batch_size=BATCH_SIZE):\n",
    "    inputs = tokenize_batch(texts)\n",
    "    dataset = TensorDataset(inputs[\"input_ids\"], inputs[\"attention_mask\"])\n",
    "    return DataLoader(dataset, batch_size=batch_size)\n",
    "\n",
    "# Predict function\n",
    "def predict_labels(texts):\n",
    "    dataloader = create_dataloader(texts)\n",
    "    all_predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids, attention_mask = [b.to(device) for b in batch]\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            predictions = torch.argmax(outputs.logits, dim=1).cpu().numpy()\n",
    "            all_predictions.extend(predictions)\n",
    "\n",
    "    return all_predictions\n",
    "\n",
    "# Run predictions in batches\n",
    "GH[\"predicted_label\"] = predict_labels(GH[\"commit_message\"].tolist())\n",
    "GH[\"predicted_label\"] = GH[\"predicted_label\"].map(label_mapping)\n",
    "\n",
    "\n",
    "# Save results\n",
    "GH.to_csv(\"GH_commit_predictions.csv\", index=False)\n",
    "\n",
    "print(\"âœ… Predictions saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "63e26ad7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8de8871e44ac4605bfc6d8bf00109f97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1f638db2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RepoUrl('https://huggingface.co/eyinlojuoluwa/distilbert-base-uncased-commit_labeller', endpoint='https://huggingface.co', repo_type='model', repo_id='eyinlojuoluwa/distilbert-base-uncased-commit_labeller')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import create_repo\n",
    "\n",
    "repo_name = \"distilbert-base-uncased-commit_labeller\"  # Change this to your preferred repository name\n",
    "create_repo(repo_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f2f5e888",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91506fa382314bac8349047ae6e04633",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "191c764a50084101998ef9504cd012db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/eyinlojuoluwa/distilbert-base-uncased-commit_labeller/commit/3b383bd64f5eeef24b843e255c8bcd080e85cbaa', commit_message='Upload tokenizer', commit_description='', oid='3b383bd64f5eeef24b843e255c8bcd080e85cbaa', pr_url=None, repo_url=RepoUrl('https://huggingface.co/eyinlojuoluwa/distilbert-base-uncased-commit_labeller', endpoint='https://huggingface.co', repo_type='model', repo_id='eyinlojuoluwa/distilbert-base-uncased-commit_labeller'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import DistilBertForSequenceClassification, DistilBertTokenizer, Trainer, TrainingArguments\n",
    "\n",
    "# Replace these with your actual paths\n",
    "model_dir = \"./final_model\"\n",
    "\n",
    "# Push model, tokenizer, and config to Hugging Face Hub\n",
    "model = DistilBertForSequenceClassification.from_pretrained(model_dir)\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(model_dir)\n",
    "\n",
    "model.push_to_hub(repo_name)\n",
    "tokenizer.push_to_hub(repo_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1b1e8255",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/eyinlojuoluwa/distilbert-base-uncased-commit_labeller/commit/3b383bd64f5eeef24b843e255c8bcd080e85cbaa', commit_message='Upload config', commit_description='', oid='3b383bd64f5eeef24b843e255c8bcd080e85cbaa', pr_url=None, repo_url=RepoUrl('https://huggingface.co/eyinlojuoluwa/distilbert-base-uncased-commit_labeller', endpoint='https://huggingface.co', repo_type='model', repo_id='eyinlojuoluwa/distilbert-base-uncased-commit_labeller'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = model.config\n",
    "config.push_to_hub(repo_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cd98767e",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {\n",
    "    0: 'add dependency',\n",
    "    1: 'external documentation',\n",
    "    2: 'input data',\n",
    "    3: 'internal documentation',\n",
    "    4: 'model structure',\n",
    "    5: 'output data',\n",
    "    6: 'parameter tuning',\n",
    "    7: 'pipeline performance',\n",
    "    8: 'preprocessing',\n",
    "    9: 'project metadata',\n",
    "    10: 'remove dependency',\n",
    "    11: 'sharing',\n",
    "    12: 'training infrastructure',\n",
    "    13: 'update dependency',\n",
    "    14: 'validation infrastructure'\n",
    "}\n",
    "\n",
    "# Create the mappings for id2label and label2id\n",
    "id2label = {i: label for i, label in label_map.items()}\n",
    "label2id = {label: i for i, label in label_map.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e9b1f484",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertConfig\n",
    "\n",
    "# Load the model configuration\n",
    "config = DistilBertConfig.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# Set the label mappings\n",
    "config.id2label = id2label\n",
    "config.label2id = label2id\n",
    "\n",
    "# Save the updated config file to the model directory\n",
    "config.save_pretrained(\"./final_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b33897a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/eyinlojuoluwa/distilbert-base-uncased-commit_labeller/commit/d86fe1d49f9303b8c11043adbca4c6ab79f53592', commit_message='Upload tokenizer', commit_description='', oid='d86fe1d49f9303b8c11043adbca4c6ab79f53592', pr_url=None, repo_url=RepoUrl('https://huggingface.co/eyinlojuoluwa/distilbert-base-uncased-commit_labeller', endpoint='https://huggingface.co', repo_type='model', repo_id='eyinlojuoluwa/distilbert-base-uncased-commit_labeller'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import DistilBertForSequenceClassification, DistilBertTokenizer, Trainer, TrainingArguments\n",
    "\n",
    "# Replace these with your actual paths\n",
    "model_dir = \"./final_model\"\n",
    "\n",
    "# Push model, tokenizer, and config to Hugging Face Hub\n",
    "model = DistilBertForSequenceClassification.from_pretrained(model_dir)\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(model_dir)\n",
    "\n",
    "model.push_to_hub(repo_name)\n",
    "tokenizer.push_to_hub(repo_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5ae5ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
