{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "00114df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import re\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments, DistilBertConfig\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import wordnet\n",
    "import random\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56d2504d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Weights: tensor([4.6842, 0.1776, 2.5429, 2.5429, 0.4564, 4.9444, 2.8710, 2.6970, 1.4590,\n",
      "        0.6312, 5.9333, 0.7008, 1.6182, 5.2353, 1.7115])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/adekunleajibode/anaconda3/lib/python3.11/site-packages/pyarrow/pandas_compat.py:373: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if _pandas_api.is_sparse(col):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/854 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/214 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/267 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 854, Validation samples: 214, Test samples: 267\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/854 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/267 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4p/p4dff74s5rb42f27sxjfg97r0000gn/T/ipykernel_20304/3335869837.py:87: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  device = torch.device(\"mps\" if torch.has_mps else \"cpu\")\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/adekunleajibode/anaconda3/lib/python3.11/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1060' max='1060' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1060/1060 1:56:52, Epoch 19/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.683000</td>\n",
       "      <td>2.689718</td>\n",
       "      <td>0.258427</td>\n",
       "      <td>0.661099</td>\n",
       "      <td>0.258427</td>\n",
       "      <td>0.269544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.648800</td>\n",
       "      <td>2.617299</td>\n",
       "      <td>0.483146</td>\n",
       "      <td>0.591568</td>\n",
       "      <td>0.483146</td>\n",
       "      <td>0.416768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.523200</td>\n",
       "      <td>2.461237</td>\n",
       "      <td>0.501873</td>\n",
       "      <td>0.675846</td>\n",
       "      <td>0.501873</td>\n",
       "      <td>0.473595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.307100</td>\n",
       "      <td>2.319612</td>\n",
       "      <td>0.561798</td>\n",
       "      <td>0.629688</td>\n",
       "      <td>0.561798</td>\n",
       "      <td>0.522000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.170400</td>\n",
       "      <td>2.211452</td>\n",
       "      <td>0.565543</td>\n",
       "      <td>0.637473</td>\n",
       "      <td>0.565543</td>\n",
       "      <td>0.533553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.032000</td>\n",
       "      <td>2.117745</td>\n",
       "      <td>0.558052</td>\n",
       "      <td>0.644434</td>\n",
       "      <td>0.558052</td>\n",
       "      <td>0.547906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.899700</td>\n",
       "      <td>2.056645</td>\n",
       "      <td>0.565543</td>\n",
       "      <td>0.634916</td>\n",
       "      <td>0.565543</td>\n",
       "      <td>0.559691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.787000</td>\n",
       "      <td>1.991419</td>\n",
       "      <td>0.580524</td>\n",
       "      <td>0.648735</td>\n",
       "      <td>0.580524</td>\n",
       "      <td>0.573389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.693000</td>\n",
       "      <td>1.944241</td>\n",
       "      <td>0.576779</td>\n",
       "      <td>0.654627</td>\n",
       "      <td>0.576779</td>\n",
       "      <td>0.577911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.575400</td>\n",
       "      <td>1.897902</td>\n",
       "      <td>0.584270</td>\n",
       "      <td>0.625230</td>\n",
       "      <td>0.584270</td>\n",
       "      <td>0.586716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.479400</td>\n",
       "      <td>1.868127</td>\n",
       "      <td>0.584270</td>\n",
       "      <td>0.621970</td>\n",
       "      <td>0.584270</td>\n",
       "      <td>0.586538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.408300</td>\n",
       "      <td>1.839752</td>\n",
       "      <td>0.580524</td>\n",
       "      <td>0.638617</td>\n",
       "      <td>0.580524</td>\n",
       "      <td>0.592446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1.334300</td>\n",
       "      <td>1.820256</td>\n",
       "      <td>0.591760</td>\n",
       "      <td>0.642241</td>\n",
       "      <td>0.591760</td>\n",
       "      <td>0.600544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.282700</td>\n",
       "      <td>1.803586</td>\n",
       "      <td>0.595506</td>\n",
       "      <td>0.633201</td>\n",
       "      <td>0.595506</td>\n",
       "      <td>0.598984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.222300</td>\n",
       "      <td>1.787956</td>\n",
       "      <td>0.591760</td>\n",
       "      <td>0.638247</td>\n",
       "      <td>0.591760</td>\n",
       "      <td>0.599534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>1.195800</td>\n",
       "      <td>1.779567</td>\n",
       "      <td>0.588015</td>\n",
       "      <td>0.632517</td>\n",
       "      <td>0.588015</td>\n",
       "      <td>0.594756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>1.144000</td>\n",
       "      <td>1.776379</td>\n",
       "      <td>0.588015</td>\n",
       "      <td>0.635889</td>\n",
       "      <td>0.588015</td>\n",
       "      <td>0.595801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>1.102000</td>\n",
       "      <td>1.772482</td>\n",
       "      <td>0.588015</td>\n",
       "      <td>0.628940</td>\n",
       "      <td>0.588015</td>\n",
       "      <td>0.594359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>1.094100</td>\n",
       "      <td>1.767704</td>\n",
       "      <td>0.584270</td>\n",
       "      <td>0.633458</td>\n",
       "      <td>0.584270</td>\n",
       "      <td>0.593078</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.read_csv(\"dataset/combined_GH_HF_manual.csv\")\n",
    "\n",
    "# Function to clean text columns\n",
    "def clean_text(text):\n",
    "    # Remove non-ASCII characters (corrupted/malformed characters)\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)  # Remove non-ASCII characters\n",
    "    return text.strip()\n",
    "\n",
    "# Apply cleaning function to relevant columns\n",
    "df['message'] = df['message'].apply(clean_text)\n",
    "df = df[df['message'].notna() & (df['message'] != '') & (df['message'].str.split().str.len() > 1)]\n",
    "df[\"label\"] = df[\"label\"].str.lower()\n",
    "\n",
    "number_of_labels = df[\"label\"].value_counts()\n",
    "number_of_labels\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "df[\"label\"] = label_encoder.fit_transform(df[\"label\"])\n",
    "\n",
    "# Compute class weights\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight=\"balanced\",\n",
    "    classes=np.unique(df[\"label\"]), \n",
    "    y=df[\"label\"]\n",
    ")\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float)\n",
    "\n",
    "# Check class weights\n",
    "print(\"Class Weights:\", class_weights)\n",
    "\n",
    "\n",
    "# First, split into training (80%) and testing (20%)\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['label'])\n",
    "\n",
    "# Then, split the training set into training (80% of 80% = 64%) and validation (20% of 80% = 16%)\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.2, random_state=42, stratify=train_df['label'])\n",
    "\n",
    "# Convert DataFrames to Hugging Face Datasets\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "val_dataset = Dataset.from_pandas(val_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "# Ensure labels are of type integer\n",
    "train_dataset = train_dataset.map(lambda e: {'label': int(e['label'])})\n",
    "val_dataset = val_dataset.map(lambda e: {'label': int(e['label'])})\n",
    "test_dataset = test_dataset.map(lambda e: {'label': int(e['label'])})\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}, Validation samples: {len(val_dataset)}, Test samples: {len(test_dataset)}\")\n",
    "\n",
    "\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Tokenization function\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"message\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "# Apply tokenization\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "class DistilBERTWithWeightedLoss(nn.Module):\n",
    "    def __init__(self, model_name, num_labels, class_weights, dropout=0.4):\n",
    "        super().__init__()\n",
    "        \n",
    "        config = DistilBertConfig.from_pretrained(\n",
    "            model_name, \n",
    "            num_labels=num_labels, \n",
    "            hidden_dropout_prob=dropout,\n",
    "            attention_probs_dropout_prob=dropout\n",
    "        )\n",
    "\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(model_name, config=config)\n",
    "        self.loss_fn = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = self.loss_fn(logits, labels)\n",
    "\n",
    "        return {\"loss\": loss, \"logits\": logits} if loss is not None else {\"logits\": logits}\n",
    "\n",
    "    \n",
    "device = torch.device(\"mps\" if torch.has_mps else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Initialize model\n",
    "num_labels = len(label_encoder.classes_)\n",
    "model = DistilBERTWithWeightedLoss(model_name, num_labels, class_weights.to(device), dropout=0.4)\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "\n",
    "# Initialize model\n",
    "num_labels = len(label_encoder.classes_)  # Count unique labels\n",
    "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = DistilBERTWithWeightedLoss(model_name, num_labels, class_weights, dropout=0.4)\n",
    "\n",
    "model.to(device)  # Move model to GPU if available\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=1,  # Keep only the best model\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,  # Lower loss is better\n",
    "    learning_rate= 1e-5,  # Lower learning rate to improve stability\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=20,  # Reduce epochs to prevent overfitting\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    save_steps=1000,  # Log less frequently to reduce clutter\n",
    "    report_to=\"none\",  # Prevents reporting to WandB, TensorBoard, etc.\n",
    "    fp16=False,  # Disable mixed precision to avoid MPS issue\n",
    "    warmup_ratio=0.1,  # Warm-up for the first 10% of training\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    gradient_accumulation_steps=2,  # Helps stabilize training\n",
    ")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=1)\n",
    "\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        labels, predictions, average='weighted', zero_division=1\n",
    "    )\n",
    "    acc = accuracy_score(labels, predictions)\n",
    "\n",
    "    return {\"accuracy\": acc, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "config = DistilBertConfig.from_pretrained(\"distilbert-base-uncased\", num_labels=num_labels)\n",
    "\n",
    "trainer.save_model(\"./final_model\")\n",
    "tokenizer.save_pretrained(\"./final_model\")\n",
    "config.save_pretrained(\"./final_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a99d21f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
